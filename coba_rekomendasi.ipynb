{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrsL21pdp4sF",
        "outputId": "9da4e90d-bf1e-4a7c-aa78-671c0ff1a8e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Laptop Recommendation Chatbot! Type 'exit' to quit.\n",
            "Enter your preferences (e.g., processor, max price, min RAM): AMD, 3000, 16\n",
            "Recommended laptops:\n",
            "Asus Rog Strix with AMD Ryzen 1700 3GHz\n",
            "Asus Rog Strix with AMD Ryzen 1700 3GHz\n",
            "Lenovo IdeaPad Y700-15ACZ with AMD FX 8800P 2.1GHz\n",
            "Enter your preferences (e.g., processor, max price, min RAM): exut\n",
            "Incomplete input. Please provide processor, max price, and min RAM.\n",
            "Enter your preferences (e.g., processor, max price, min RAM): exit\n",
            "Goodbye!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load laptop data from CSV file into a DataFrame\n",
        "def load_laptop_data_from_csv(file_path):\n",
        "    laptops = pd.read_csv('/content/laptop.csv', encoding='ISO-8859-1')\n",
        "    return laptops\n",
        "\n",
        "# Recommend laptops based on user preferences\n",
        "def recommend_laptops(preferences, laptops):\n",
        "    # Filter laptops based on user preferences\n",
        "    recommended_laptops = laptops[\n",
        "        (laptops['Price_euros'] <= preferences['Price_euros']) &\n",
        "        (laptops['Cpu_Com'] == preferences['Cpu_Com']) &\n",
        "        (laptops['Ram'] >= preferences['Ram'])\n",
        "    ]\n",
        "    return recommended_laptops\n",
        "\n",
        "# Main function to interact with the user\n",
        "def main():\n",
        "    csv_file_path = '/content/laptop.csv'  # Change this to your laptop data CSV file path\n",
        "    laptops = load_laptop_data_from_csv(csv_file_path)\n",
        "\n",
        "    print(\"Welcome to the Laptop Recommendation Chatbot! Type 'exit' to quit.\")\n",
        "    while True:\n",
        "        user_input = input(\"Enter your preferences (e.g., processor, max price, min RAM): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Parse user preferences\n",
        "        preferences = parse_user_input(user_input)\n",
        "\n",
        "        # If user input is incomplete, prompt again\n",
        "        if preferences is None:\n",
        "            print(\"Incomplete input. Please provide processor, max price, and min RAM.\")\n",
        "            continue\n",
        "\n",
        "        # Recommend laptops based on preferences\n",
        "        recommended_laptops = recommend_laptops(preferences, laptops)\n",
        "\n",
        "        if not recommended_laptops.empty:\n",
        "            print(\"Recommended laptops:\")\n",
        "            for index, laptop in recommended_laptops.iterrows():\n",
        "                print(laptop['Company'], laptop['Product'],\"with\", laptop['Cpu'])\n",
        "        else:\n",
        "            print(\"No laptops found matching your criteria. Please adjust your preferences.\")\n",
        "\n",
        "# Function to parse user preferences\n",
        "def parse_user_input(input_str):\n",
        "    # Split the input string and extract preferences\n",
        "    preferences = {}\n",
        "    input_parts = input_str.split(',')\n",
        "    if len(input_parts) != 3:\n",
        "        return None\n",
        "    preferences['Cpu_Com'] = input_parts[0].strip()\n",
        "    preferences['Price_euros'] = float(input_parts[1].strip())\n",
        "    preferences['Ram'] = int(input_parts[2].strip())\n",
        "    return preferences\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Ja8Yz50eoRu"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('/content/tokenizer.pickle', 'rb') as handle:\n",
        "    loaded_tokenizer = pickle.load(handle)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZvFtvSGRJrE",
        "outputId": "220d8dc4-5cd5-446d-caea-8fa3fb08bf92"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Welcome to the Laptop Recommendation Chatbot! Type 'exit' to quit.\n",
            "1/1 [==============================] - 0s 419ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7e19105d3eb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 415ms/step\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7e190ce801f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 415ms/step\n",
            "CPU Predictions: [[9.9999571e-01 4.2830916e-06]]\n",
            "Price Predictions: [[1.92719614e-04 1.53274887e-04 9.27573186e-04 7.55240080e-06\n",
            "  3.59654380e-03 4.15230788e-05 5.81002934e-03 3.34111210e-05\n",
            "  1.20978042e-01 3.25515836e-01 4.29864507e-03 9.01184883e-03\n",
            "  4.55504796e-03 3.12995642e-01 7.41977827e-04 3.11018638e-02\n",
            "  9.97553725e-05 1.12099435e-04 6.36124164e-02 4.79948521e-02\n",
            "  9.19011887e-03 2.51608598e-03 1.79620463e-06 3.22558444e-05\n",
            "  8.95237736e-03 9.80821205e-04 3.68185085e-03 3.49532552e-02\n",
            "  9.09290520e-06 5.11575444e-03 5.24496078e-04 8.00520240e-04\n",
            "  3.96262098e-04 1.71005559e-05 4.42696823e-04 3.49155045e-04\n",
            "  5.54633225e-05 9.18234264e-07 1.99280083e-04]]\n",
            "RAM Predictions: [[2.4092192e-06 8.7889441e-04 1.0519500e-03 9.9506539e-01 1.1622868e-04\n",
            "  2.6477212e-03 1.1397953e-04 7.2263210e-05 5.1211264e-05]]\n",
            "CPU Index: 0\n",
            "Price Index: 9\n",
            "RAM Index: 3\n",
            "Recommended laptops:\n",
            "HP 15-BW094nd (A6-9220/8GB/128GB/W10)\n",
            "Acer ES1-523-84K7 (A8-7410/8GB/256GB/FHD/W10)\n",
            "Lenovo IdeaPad 320-15ABR\n",
            "Acer Aspire ES1-523\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the tokenizer\n",
        "with open('/content/tokenizer.pickle', 'rb') as handle:\n",
        "    loaded_tokenizer = pickle.load(handle)\n",
        "\n",
        "# Load NLP models\n",
        "cpu_model = tf.keras.models.load_model('/content/model_cpu.keras')\n",
        "price_model = tf.keras.models.load_model('/content/model_price.keras')\n",
        "ram_model = tf.keras.models.load_model('/content/model_ram.keras')\n",
        "\n",
        "# Load laptop data from CSV file into a DataFrame\n",
        "def load_laptop_data_from_csv(file_path):\n",
        "    laptops = pd.read_csv('/content/laptop.csv', encoding='ISO-8859-1')\n",
        "    return laptops\n",
        "\n",
        "# Recommend laptops based on user preferences\n",
        "def recommend_laptops(preferences, laptops):\n",
        "    # Filter laptops based on user preferences\n",
        "    recommended_laptops = laptops[\n",
        "        (laptops['Price_euros'] <= preferences['Price_euros']) &\n",
        "        (laptops['Cpu_Com'] == preferences['Cpu_Com']) &\n",
        "        (laptops['Ram'] >= preferences['Ram'])\n",
        "    ]\n",
        "    return recommended_laptops\n",
        "\n",
        "# Main function to interact with the user\n",
        "def main():\n",
        "    csv_file_path = '/content/laptop.csv'  # Change this to your laptop data CSV file path\n",
        "    laptops = load_laptop_data_from_csv(csv_file_path)\n",
        "\n",
        "    print(\"Welcome to the Laptop Recommendation Chatbot! Type 'exit' to quit.\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"Enter your preferences: \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            print(\"Goodbye!\")\n",
        "            break\n",
        "\n",
        "        # Parse user preferences using NLP model\n",
        "        preferences = parse_user_input(user_input)\n",
        "\n",
        "        # If user input is incomplete, prompt again\n",
        "        if preferences is None:\n",
        "            print(\"Incomplete input. Please provide processor, max price, and min RAM.\")\n",
        "            #print(preferences)\n",
        "            print(\"cek\")\n",
        "            continue\n",
        "\n",
        "        # Recommend laptops based on preferences\n",
        "        recommended_laptops = recommend_laptops(preferences, laptops)\n",
        "\n",
        "        if not recommended_laptops.empty:\n",
        "            print(\"Recommended laptops:\")\n",
        "            for index, laptop in recommended_laptops.iterrows():\n",
        "                print(laptop['Company'], laptop['Product'])\n",
        "        else:\n",
        "            print(\"No laptops found matching your criteria. Please adjust your preferences.\")\n",
        "            #print(preferences)\n",
        "\n",
        "# Function to parse user preferences using NLP model\n",
        "def parse_user_input(input_str):\n",
        "    # Process user input\n",
        "    input_sequence = loaded_tokenizer.texts_to_sequences([input_str])\n",
        "    input_sequence = pad_sequences(input_sequence, maxlen=21, padding='post')  # Adjust maxlen here\n",
        "\n",
        "    # Make predictions using NLP models\n",
        "    cpu_predictions = cpu_model.predict(input_sequence)\n",
        "    price_predictions = price_model.predict(input_sequence)\n",
        "    ram_predictions = ram_model.predict(input_sequence)\n",
        "\n",
        "    cpu_index = np.argmax(cpu_predictions[0])\n",
        "    price_index = np.argmax(price_predictions[0])\n",
        "    ram_index = np.argmax(ram_predictions[0])\n",
        "\n",
        "    print(\"CPU Predictions:\", cpu_predictions)\n",
        "    print(\"Price Predictions:\", price_predictions)\n",
        "    print(\"RAM Predictions:\", ram_predictions)\n",
        "    print(\"CPU Index:\", cpu_index)\n",
        "    print(\"Price Index:\", price_index)\n",
        "    print(\"RAM Index:\", ram_index)\n",
        "\n",
        "\n",
        "\n",
        "    # Map indices to actual values\n",
        "    cpu_map = {0: 'AMD', 1: 'Intel'}\n",
        "    price_map =  {0: 50, 1: 100, 2: 150, 3: 200, 4: 250, 5: 300, 6: 350, 7: 400, 8: 450, 9: 500, 10: 600, 11: 650, 12: 670, 13: 700, 14: 800, 15: 830, 16: 850, 17: 880, 18: 900, 19: 950, 20: 970, 21: 1000, 22: 1100, 23: 1150, 24: 1200, 25: 1250, 26: 1270, 27: 1300, 28: 1400, 29: 1500, 30: 1600, 31: 1650, 32: 1700, 33: 1800, 34: 2000, 35: 2100, 36: 2150, 37: 2500, 38: 3500}\n",
        "    ram_map = {0: 2, 1: 4, 2: 6, 3: 8, 4: 12, 5: 16, 6: 24, 7: 32, 8: 64}\n",
        "\n",
        "    try:\n",
        "        preferences = {\n",
        "            'Cpu_Com': cpu_map[cpu_index],\n",
        "            'Price_euros': price_map[price_index],\n",
        "            'Ram': ram_map[ram_index]\n",
        "        }\n",
        "\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(\"Error:\", e)\n",
        "        preferences = None\n",
        "\n",
        "    # Check if all preferences are extracted\n",
        "    if preferences is None or None in preferences.values():\n",
        "        return None\n",
        "    else:\n",
        "        return preferences\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}